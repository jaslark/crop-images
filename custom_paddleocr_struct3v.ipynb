{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaslark/crop-images/blob/main/custom_paddleocr_struct3v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Zn9f_zoNVr"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install PaddlePaddle and PaddleOCR\n",
        "# 1. Upgrade pip\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# 2. Install PaddlePaddle GPU\n",
        "!pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n",
        "\n",
        "# 3. Install PaddleOCR and the missing dependencies causing your error\n",
        "!pip install paddleocr==2.9.1  # Suggesting 2.9.1 as it is very stable\n",
        "!pip install langchain langchain-community # <--- THIS FIXES YOUR SPECIFIC ERROR\n",
        "\n",
        "# 4. Install helpers\n",
        "!pip install pdf2image PyMuPDF Pillow opencv-python-headless\n",
        "!apt-get update\n",
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Verify Installation\n",
        "import paddle\n",
        "print(f\"PaddlePaddle Version: {paddle.__version__}\")\n",
        "print(f\"GPU Available: {paddle.is_compiled_with_cuda()}\")\n",
        "print(f\"GPU Device: {paddle.device.get_device()}\")"
      ],
      "metadata": {
        "id": "umPkHJ6P1jiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Import Libraries and Define Main Class\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Union, Optional\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_DIR = \"/content/pp_structure_output\"\n",
        "CROP_DIR = os.path.join(OUTPUT_DIR, \"cropped_images\")\n",
        "JSON_DIR = os.path.join(OUTPUT_DIR, \"json_results\")\n",
        "ZIP_DIR = os.path.join(OUTPUT_DIR, \"zip_files\")\n",
        "\n",
        "for dir_path in [OUTPUT_DIR, CROP_DIR, JSON_DIR, ZIP_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"Output directories created successfully!\")"
      ],
      "metadata": {
        "id": "eX5uqHb61nqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# UPDATED CELL 4 & 5: Robust Processor (PDF Support + Memory Reset)\n",
        "# ==========================================\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import sys\n",
        "import paddle\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# --- ğŸ”´ CRITICAL FIX: FORCE RESET GLOBAL PROCESSOR ---\n",
        "# This ensures the old instance (without PDF support) is deleted and replaced.\n",
        "if '_GLOBAL_PROCESSOR' in globals():\n",
        "    print(\"ğŸ”„ Resetting Processor Instance to apply updates...\")\n",
        "    _GLOBAL_PROCESSOR = None\n",
        "\n",
        "class PPStructureV3Processor:\n",
        "    def __init__(self):\n",
        "        self.ocr_engine = None\n",
        "\n",
        "        print(\"âš™ï¸ Initializing Processor Components...\")\n",
        "        try:\n",
        "            from paddleocr import PaddleOCR\n",
        "            print(\"   â€¢ Initializing Standard PaddleOCR Engine...\")\n",
        "            # Initialize OCR engine\n",
        "            self.ocr_engine = PaddleOCR(\n",
        "                use_angle_cls=True,\n",
        "                lang='en',\n",
        "                use_gpu=True,\n",
        "                show_log=False\n",
        "            )\n",
        "            print(\"   âœ… PaddleOCR Engine loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Critical Error initializing OCR: {e}\")\n",
        "\n",
        "    def process_image_array(self, img_array):\n",
        "        \"\"\"Runs OCR on a single numpy image array\"\"\"\n",
        "        if self.ocr_engine:\n",
        "            try:\n",
        "                # cls=True enables angle classification\n",
        "                result = self.ocr_engine.ocr(img_array, cls=True)\n",
        "                return self._convert_paddleocr_result(result)\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ OCR Predict Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    def _convert_paddleocr_result(self, result):\n",
        "        \"\"\"Standardizes PaddleOCR output format\"\"\"\n",
        "        converted = {'text_det_boxes': [], 'rec_texts': [], 'rec_scores': []}\n",
        "\n",
        "        # Handle empty results\n",
        "        if not result or result[0] is None:\n",
        "            return []\n",
        "\n",
        "        # PaddleOCR structure: [ [ [box], [text, score] ], ... ]\n",
        "        for line in result[0]:\n",
        "            converted['text_det_boxes'].append(line[0])\n",
        "            converted['rec_texts'].append(line[1][0])\n",
        "            converted['rec_scores'].append(line[1][1])\n",
        "        return [converted]\n",
        "\n",
        "    def extract_text_regions(self, result, original_image, filename_prefix, page_num=0):\n",
        "        \"\"\"Crops text regions from the image based on OCR detection\"\"\"\n",
        "        cropped_images = []\n",
        "        text_regions = []\n",
        "\n",
        "        if not result:\n",
        "            return [], []\n",
        "\n",
        "        if isinstance(result, list) and len(result) > 0:\n",
        "            result = result[0]\n",
        "\n",
        "        boxes = result.get('text_det_boxes', [])\n",
        "        texts = result.get('rec_texts', [])\n",
        "        scores = result.get('rec_scores', [])\n",
        "\n",
        "        CROP_DIR = \"/content/pp_structure_output/cropped_images\"\n",
        "        os.makedirs(CROP_DIR, exist_ok=True)\n",
        "\n",
        "        for i, box in enumerate(boxes):\n",
        "            try:\n",
        "                box = np.array(box).astype(np.int32)\n",
        "\n",
        "                # Filter noise\n",
        "                if box.size < 8: continue\n",
        "\n",
        "                xs = [pt[0] for pt in box]\n",
        "                ys = [pt[1] for pt in box]\n",
        "\n",
        "                # Add padding\n",
        "                pad = 2\n",
        "                x_min = max(0, int(min(xs)) - pad)\n",
        "                x_max = min(original_image.shape[1], int(max(xs)) + pad)\n",
        "                y_min = max(0, int(min(ys)) - pad)\n",
        "                y_max = min(original_image.shape[0], int(max(ys)) + pad)\n",
        "\n",
        "                if x_max <= x_min or y_max <= y_min: continue\n",
        "\n",
        "                crop = original_image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "                # Naming convention: filename_page_region.png\n",
        "                crop_name = f\"{filename_prefix}_p{page_num+1}_r{i}.png\"\n",
        "                crop_path = os.path.join(CROP_DIR, crop_name)\n",
        "\n",
        "                # Save crop\n",
        "                cv2.imwrite(crop_path, crop)\n",
        "\n",
        "                txt = texts[i] if i < len(texts) else \"\"\n",
        "                conf = scores[i] if i < len(scores) else 0.0\n",
        "\n",
        "                info = {\n",
        "                    \"page\": page_num + 1,\n",
        "                    \"region_index\": i,\n",
        "                    \"text\": txt,\n",
        "                    \"confidence\": float(conf),\n",
        "                    \"crop_image\": crop_name\n",
        "                }\n",
        "                text_regions.append(info)\n",
        "                cropped_images.append({'path': crop_path, 'filename': crop_name})\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return cropped_images, text_regions\n",
        "\n",
        "    def create_output_package(self, base_filename, all_crops, all_regions):\n",
        "        \"\"\"Packages everything into a ZIP file\"\"\"\n",
        "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        JSON_DIR = \"/content/pp_structure_output/json_results\"\n",
        "        ZIP_DIR = \"/content/pp_structure_output/zip_files\"\n",
        "        os.makedirs(JSON_DIR, exist_ok=True)\n",
        "        os.makedirs(ZIP_DIR, exist_ok=True)\n",
        "\n",
        "        json_name = f\"{base_filename}_{ts}.json\"\n",
        "        json_path = os.path.join(JSON_DIR, json_name)\n",
        "        zip_path = os.path.join(ZIP_DIR, f\"{base_filename}_{ts}.zip\")\n",
        "\n",
        "        # Create Master JSON\n",
        "        output_data = {\n",
        "            'filename': base_filename,\n",
        "            'total_pages': max([r['page'] for r in all_regions]) if all_regions else 1,\n",
        "            'total_regions': len(all_regions),\n",
        "            'regions': all_regions\n",
        "        }\n",
        "\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Create ZIP\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "            zf.write(json_path, json_name)\n",
        "            for crop in all_crops:\n",
        "                zf.write(crop['path'], f\"crops/{crop['filename']}\")\n",
        "\n",
        "        return zip_path\n",
        "\n",
        "# --- Main Processing Logic ---\n",
        "def process_document(file_path):\n",
        "    global _GLOBAL_PROCESSOR\n",
        "\n",
        "    # Initialize singleton if not present\n",
        "    if _GLOBAL_PROCESSOR is None:\n",
        "        _GLOBAL_PROCESSOR = PPStructureV3Processor()\n",
        "\n",
        "    filename = os.path.basename(file_path)\n",
        "    base_name = Path(file_path).stem\n",
        "    ext = Path(file_path).suffix.lower()\n",
        "\n",
        "    print(f\"ğŸš€ Processing: {filename}\")\n",
        "\n",
        "    all_crops = []\n",
        "    all_regions = []\n",
        "\n",
        "    # 1. Load Document (PDF or Image)\n",
        "    images_to_process = [] # List of tuples: (numpy_image, page_index)\n",
        "\n",
        "    if ext == '.pdf':\n",
        "        print(\"   ğŸ“„ PDF detected. Converting pages to images (this may take a moment)...\")\n",
        "        try:\n",
        "            # Convert PDF pages to images\n",
        "            pil_images = convert_from_path(file_path)\n",
        "            for idx, pil_img in enumerate(pil_images):\n",
        "                # Convert PIL (RGB) to OpenCV (BGR)\n",
        "                opencv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
        "                images_to_process.append((opencv_img, idx))\n",
        "            print(f\"   âœ… Converted {len(images_to_process)} pages successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error converting PDF: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        # Standard Image\n",
        "        img = cv2.imread(file_path)\n",
        "        if img is None:\n",
        "            print(\"âŒ Error: Could not load image file.\")\n",
        "            return None\n",
        "        images_to_process.append((img, 0))\n",
        "\n",
        "    # 2. Iterate and Process Each Page\n",
        "    for img, page_idx in images_to_process:\n",
        "        print(f\"   â€¢ Processing Page {page_idx + 1}...\")\n",
        "\n",
        "        # A. Detect Text\n",
        "        raw_results = _GLOBAL_PROCESSOR.process_image_array(img)\n",
        "\n",
        "        # B. Extract Crops & Data\n",
        "        crops, regions = _GLOBAL_PROCESSOR.extract_text_regions(\n",
        "            raw_results, img, base_name, page_num=page_idx\n",
        "        )\n",
        "\n",
        "        all_crops.extend(crops)\n",
        "        all_regions.extend(regions)\n",
        "\n",
        "    # 3. Package Results\n",
        "    if not all_regions:\n",
        "        print(\"âš ï¸ No text found in document.\")\n",
        "\n",
        "    zip_file = _GLOBAL_PROCESSOR.create_output_package(\n",
        "        base_name, all_crops, all_regions\n",
        "    )\n",
        "\n",
        "    return zip_file\n",
        "\n",
        "print(\"âœ“ Robust Processor Loaded (Memory Reset & PDF Support Active)\")"
      ],
      "metadata": {
        "id": "lDex7Bu81rjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Upload and Process Interface\n",
        "from google.colab import files\n",
        "\n",
        "def upload_and_process():\n",
        "    \"\"\"Upload files and process them\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"       PP-StructureV3 Document Processor\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nğŸ“¤ Please upload your image or PDF file...\")\n",
        "    print(\"   Supported formats: JPG, PNG, PDF, BMP, TIFF\\n\")\n",
        "\n",
        "    # Upload file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"âŒ No file uploaded!\")\n",
        "        return None\n",
        "\n",
        "    for filename, content in uploaded.items():\n",
        "        print(f\"\\nğŸ“„ Processing: {filename}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Save uploaded file\n",
        "        input_path = f\"/content/{filename}\"\n",
        "        with open(input_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        # Process the document\n",
        "        try:\n",
        "            zip_path = process_document(input_path)\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"âœ… Processing Complete!\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            # Show results summary\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "                file_list = zf.namelist()\n",
        "                crop_count = len([f for f in file_list if f.startswith('cropped_images/') and f != 'cropped_images/'])\n",
        "\n",
        "                print(f\"\\nğŸ“Š Results Summary:\")\n",
        "                print(f\"   â€¢ Cropped text regions: {crop_count}\")\n",
        "                print(f\"   â€¢ Output ZIP: {os.path.basename(zip_path)}\")\n",
        "\n",
        "                # Show JSON preview\n",
        "                for name in file_list:\n",
        "                    if name.endswith('.json'):\n",
        "                        with zf.open(name) as jf:\n",
        "                            json_data = json.load(jf)\n",
        "                            print(f\"\\nğŸ“ Detected Text Preview (first 5 regions):\")\n",
        "                            for i, region in enumerate(json_data.get('text_regions', [])[:5]):\n",
        "                                text = region.get('text', 'N/A')\n",
        "                                conf = region.get('confidence', 0)\n",
        "                                print(f\"   {i+1}. \\\"{text[:50]}{'...' if len(text) > 50 else ''}\\\" (conf: {conf:.2f})\")\n",
        "\n",
        "            # Download the ZIP file\n",
        "            print(f\"\\nğŸ“¥ Downloading: {os.path.basename(zip_path)}\")\n",
        "            files.download(zip_path)\n",
        "\n",
        "            return zip_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ Error processing file: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "\n",
        "print(\"âœ“ Upload interface ready!\")"
      ],
      "metadata": {
        "id": "ze990eB61wy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: MAIN EXECUTION - Run this cell to process your documents!\n",
        "\n",
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘          ğŸ“„ Document Text Detector & Extractor (2025)            â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                  â•‘\n",
        "â•‘  ğŸ¯ Features:                                                    â•‘\n",
        "â•‘     âœ“ Text Detection (locate text in images)                     â•‘\n",
        "â•‘     âœ“ Text Recognition (OCR - read the text)                     â•‘\n",
        "â•‘     âœ“ Automatic text angle correction                            â•‘\n",
        "â•‘     âœ“ Cropped images for each text region                        â•‘\n",
        "â•‘     âœ“ JSON with bounding boxes, text & confidence                â•‘\n",
        "â•‘     âœ“ Single ZIP download with everything                        â•‘\n",
        "â•‘                                                                  â•‘\n",
        "â•‘  ğŸ“ Supported formats: JPG, PNG, PDF, BMP, TIFF, WEBP            â•‘\n",
        "â•‘                                                                  â•‘\n",
        "â•‘  ğŸ“¦ Output ZIP contains:                                         â•‘\n",
        "â•‘     â””â”€â”€ original/          â†’ Your uploaded file                  â•‘\n",
        "â•‘     â””â”€â”€ cropped_images/    â†’ Individual text region crops        â•‘\n",
        "â•‘     â””â”€â”€ results/           â†’ JSON with all detection data        â•‘\n",
        "â•‘                                                                  â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n",
        "\n",
        "# Process documents\n",
        "result = upload_and_process()"
      ],
      "metadata": {
        "id": "GowfoBRu1ywF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}